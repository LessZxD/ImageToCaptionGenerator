{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":7054508,"sourceType":"datasetVersion","datasetId":4060289}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\nfrom tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\nfrom tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textwrap import wrap\n\nplt.rcParams['font.size'] = 12\nsns.set_style(\"dark\")\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T11:56:57.861216Z","iopub.execute_input":"2023-12-10T11:56:57.861654Z","iopub.status.idle":"2023-12-10T11:57:13.419429Z","shell.execute_reply.started":"2023-12-10T11:56:57.861622Z","shell.execute_reply":"2023-12-10T11:57:13.417302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Reading","metadata":{}},{"cell_type":"code","source":"image_path = '../input/flickr8k/Images'","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:13.421795Z","iopub.execute_input":"2023-12-10T11:57:13.424711Z","iopub.status.idle":"2023-12-10T11:57:13.431557Z","shell.execute_reply.started":"2023-12-10T11:57:13.424665Z","shell.execute_reply":"2023-12-10T11:57:13.429790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/flickr8k/captions.txt\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:13.434084Z","iopub.execute_input":"2023-12-10T11:57:13.434831Z","iopub.status.idle":"2023-12-10T11:57:13.566314Z","shell.execute_reply.started":"2023-12-10T11:57:13.434793Z","shell.execute_reply":"2023-12-10T11:57:13.563969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readImage(path,img_size=224):\n    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    \n    return img\n\ndef display_images(temp_df):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize = (20 , 20))\n    n = 0\n    for i in range(15):\n        n+=1\n        plt.subplot(5 , 5, n)\n        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n        plt.imshow(image)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:13.569777Z","iopub.execute_input":"2023-12-10T11:57:13.570515Z","iopub.status.idle":"2023-12-10T11:57:13.579655Z","shell.execute_reply.started":"2023-12-10T11:57:13.570475Z","shell.execute_reply":"2023-12-10T11:57:13.577367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images(data.sample(15))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:13.581926Z","iopub.execute_input":"2023-12-10T11:57:13.582395Z","iopub.status.idle":"2023-12-10T11:57:17.052014Z","shell.execute_reply.started":"2023-12-10T11:57:13.582356Z","shell.execute_reply":"2023-12-10T11:57:17.051010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:17.053827Z","iopub.execute_input":"2023-12-10T11:57:17.054208Z","iopub.status.idle":"2023-12-10T11:57:17.062900Z","shell.execute_reply.started":"2023-12-10T11:57:17.054178Z","shell.execute_reply":"2023-12-10T11:57:17.061544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PreProcessed Text\n","metadata":{}},{"cell_type":"code","source":"data = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:17.064362Z","iopub.execute_input":"2023-12-10T11:57:17.064817Z","iopub.status.idle":"2023-12-10T11:57:17.241605Z","shell.execute_reply.started":"2023-12-10T11:57:17.064774Z","shell.execute_reply":"2023-12-10T11:57:17.240362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization ","metadata":{}},{"cell_type":"code","source":"\ntokenizer = Tokenizer()\n\n# Fit the tokenizer on the caption data to create a word vocabulary.\ntokenizer.fit_on_texts(captions)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Identify unique image filenames and split the dataset into training and validation sets.\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85*nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\n# Segment the data into training and validation sets based on image filenames.\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:17.243458Z","iopub.execute_input":"2023-12-10T11:57:17.243967Z","iopub.status.idle":"2023-12-10T11:57:17.995910Z","shell.execute_reply.started":"2023-12-10T11:57:17.243922Z","shell.execute_reply":"2023-12-10T11:57:17.994358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Feature Extraction","metadata":{}},{"cell_type":"code","source":"\n# Path to the downloaded DenseNet201 weights file\nweights_path = '/kaggle/input/densenet/densenet201_weights_tf_dim_ordering_tf_kernels.h5'\n\n# Load the DenseNet201 model with pre-trained ImageNet weights\ndensenet_model = DenseNet201(weights=None, include_top=True)\ndensenet_model.load_weights(weights_path)\n\n# Create a feature extraction model\nfe = Model(inputs=densenet_model.input, outputs=densenet_model.layers[-2].output)\n\n# Image directory\nimage_path = '../input/flickr8k/Images'\nimg_size = 224\n\nfeatures = {}\n\n# Assuming data['image'] contains filenames or paths of images\nfor image_filename in tqdm(data['image'].unique().tolist()):\n    img_path = os.path.join(image_path, image_filename)\n    if os.path.exists(img_path):\n        img = load_img(img_path, target_size=(img_size, img_size))\n        img = img_to_array(img)\n        img = img / 255.0  # Normalize pixel values\n        img = np.expand_dims(img, axis=0)\n        feature = fe.predict(img, verbose=0)\n        features[image_filename] = feature\n    else:\n        print(f\"Image file {image_filename} not found at {img_path}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T11:57:17.997846Z","iopub.execute_input":"2023-12-10T11:57:17.998254Z","iopub.status.idle":"2023-12-10T12:34:20.419151Z","shell.execute_reply.started":"2023-12-10T11:57:17.998209Z","shell.execute_reply":"2023-12-10T12:34:20.418208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generation","metadata":{}},{"cell_type":"code","source":"class CustomDataGenerator(Sequence):\n    \n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n                 vocab_size, max_length, features,shuffle=True):\n        # Copies the input DataFrame and sets attributes for data manipulation\n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __len__(self):\n        return self.n // self.batch_size\n    \n    def __getitem__(self,index):\n    \n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n        X1, X2, y = self.__get_data(batch)        \n        return (X1, X2), y\n    \n    def __get_data(self,batch):\n        # create empty list\n        X1, X2, y = list(), list(), list()\n        \n        images = batch[self.X_col].tolist()\n           \n        for image in images:\n            feature = self.features[image][0]\n            \n            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1,len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    # Append generated sequences to respective lists\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n        # Convert lists to numpy arrays    \n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                \n        return X1, X2, y","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:20.424511Z","iopub.execute_input":"2023-12-10T12:34:20.424933Z","iopub.status.idle":"2023-12-10T12:34:20.442228Z","shell.execute_reply.started":"2023-12-10T12:34:20.424898Z","shell.execute_reply":"2023-12-10T12:34:20.440478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"#set input layer for the model\n\ninput1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\n# Process image features through a Dense layer\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\n# Process text sequences through an Embedding layer\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n\n# Merge image and text features\nmerged = concatenate([img_features_reshaped,sentence_features],axis=1)\n\n# Apply necessary methods\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\n\n# Generate the final output predictions using a softmax activation\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1,input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy',optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:20.443868Z","iopub.execute_input":"2023-12-10T12:34:20.444268Z","iopub.status.idle":"2023-12-10T12:34:20.973877Z","shell.execute_reply.started":"2023-12-10T12:34:20.444235Z","shell.execute_reply":"2023-12-10T12:34:20.970537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:20.976196Z","iopub.execute_input":"2023-12-10T12:34:20.976712Z","iopub.status.idle":"2023-12-10T12:34:20.984184Z","shell.execute_reply.started":"2023-12-10T12:34:20.976667Z","shell.execute_reply":"2023-12-10T12:34:20.982208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Modification","metadata":{}},{"cell_type":"code","source":"plot_model(caption_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:20.985599Z","iopub.execute_input":"2023-12-10T12:34:20.986066Z","iopub.status.idle":"2023-12-10T12:34:21.329650Z","shell.execute_reply.started":"2023-12-10T12:34:20.986030Z","shell.execute_reply":"2023-12-10T12:34:21.328207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:21.332047Z","iopub.execute_input":"2023-12-10T12:34:21.333472Z","iopub.status.idle":"2023-12-10T12:34:21.387541Z","shell.execute_reply.started":"2023-12-10T12:34:21.333415Z","shell.execute_reply":"2023-12-10T12:34:21.386016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a custom data generator for training and validation\n\n# Training data generator\ntrain_generator = CustomDataGenerator(\n    df=train,X_col='image'\n    ,y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\n\n# Validation data generator\nvalidation_generator = CustomDataGenerator(\n    df=test,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:21.389950Z","iopub.execute_input":"2023-12-10T12:34:21.391276Z","iopub.status.idle":"2023-12-10T12:34:21.405718Z","shell.execute_reply.started":"2023-12-10T12:34:21.391201Z","shell.execute_reply":"2023-12-10T12:34:21.403923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"model.h5\"\ncheckpoint = ModelCheckpoint(\n    model_name,\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_best_only = True,\n    verbose=1\n                            )\n\n# Early stopping to stop training if validation loss doesn't improve after a certain number of epochs\nearlystopping = EarlyStopping(\n    monitor='val_loss', # Metric to monitor for early stopping\n    min_delta = 0, # Minimum change in the monitored quantity to qualify as improvement\n    patience = 5, # Number of epochs with no improvement after which training will be stopped\n    verbose = 1, # Verbosity mode (1: displays messages about early stopping)\n    restore_best_weights=True # Restore the weights of the best model\n                            )\n\n# Reduce learning rate when the validation loss has stopped improving\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor='val_loss', # Metric to monitor for reducing learning rate\n    patience=3, # Number of epochs to wait before applying the reduction factor\n    verbose=1, # Verbosity mode (1: displays messages about reducing learning rate)\n    factor=0.2, # Factor by which the learning rate will be reduced\n    min_lr=0.00000001  # Minimum learning rate allowed after reduction\n                                           )","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:21.408538Z","iopub.execute_input":"2023-12-10T12:34:21.409212Z","iopub.status.idle":"2023-12-10T12:34:21.418284Z","shell.execute_reply.started":"2023-12-10T12:34:21.409165Z","shell.execute_reply":"2023-12-10T12:34:21.417247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the caption generation model using the previously defined generators and callbacks\n\nhistory = caption_model.fit(\n        train_generator,\n        epochs=5,\n        validation_data=validation_generator,\n        callbacks=[checkpoint,earlystopping,learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T12:34:21.420854Z","iopub.execute_input":"2023-12-10T12:34:21.421337Z","iopub.status.idle":"2023-12-10T13:44:20.613079Z","shell.execute_reply.started":"2023-12-10T12:34:21.421301Z","shell.execute_reply":"2023-12-10T13:44:20.611529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Caption Generation ","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer,tokenizer):\n    # Iterate through the word index in the tokenizer\n    for word, index in tokenizer.word_index.items():\n        # Check if the index matches the provided integer\n        if index==integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:20.614958Z","iopub.execute_input":"2023-12-10T13:44:20.615435Z","iopub.status.idle":"2023-12-10T13:44:20.622971Z","shell.execute_reply.started":"2023-12-10T13:44:20.615400Z","shell.execute_reply":"2023-12-10T13:44:20.621454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length, features):\n    \n    # Retrieve the features of the given image   \n    feature = features[image]\n    \n    # Initialize the caption with the start token\n    in_text = \"startseq\"\n    \n    for i in range(max_length):\n        # Convert the current caption text to a sequence of word indices using the tokenizer\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], max_length)\n\n        y_pred = model.predict([feature,sequence])\n        y_pred = np.argmax(y_pred)\n        \n        word = idx_to_word(y_pred, tokenizer)\n        \n        if word is None:\n            break\n            \n        in_text+= \" \" + word\n        \n        # break loop if token ends, add endseq\n        if word == 'endseq':\n            break\n            \n    return in_text ","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:20.624975Z","iopub.execute_input":"2023-12-10T13:44:20.626795Z","iopub.status.idle":"2023-12-10T13:44:20.641381Z","shell.execute_reply.started":"2023-12-10T13:44:20.626722Z","shell.execute_reply":"2023-12-10T13:44:20.639931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Taking 15 random samples for caption prediction","metadata":{}},{"cell_type":"code","source":"samples = test.sample(15)\nsamples.reset_index(drop=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:20.642987Z","iopub.execute_input":"2023-12-10T13:44:20.643400Z","iopub.status.idle":"2023-12-10T13:44:20.660092Z","shell.execute_reply.started":"2023-12-10T13:44:20.643367Z","shell.execute_reply":"2023-12-10T13:44:20.658871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,record in samples.iterrows():\n\n    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n    img = img_to_array(img)\n    img = img/255.\n    \n    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n    samples.loc[index,'caption'] = caption","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:20.661314Z","iopub.execute_input":"2023-12-10T13:44:20.662008Z","iopub.status.idle":"2023-12-10T13:44:30.634567Z","shell.execute_reply.started":"2023-12-10T13:44:20.661974Z","shell.execute_reply":"2023-12-10T13:44:30.633552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"display_images(samples)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:30.636275Z","iopub.execute_input":"2023-12-10T13:44:30.636964Z","iopub.status.idle":"2023-12-10T13:44:33.979767Z","shell.execute_reply.started":"2023-12-10T13:44:30.636909Z","shell.execute_reply":"2023-12-10T13:44:33.978779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check lengths of the lists\nprint(f\"Number of images (generated): {len(generated_captions)}\")\nprint(f\"Number of images (reference): {len(reference_captions)}\")\n\n# Check lengths of sublists within generated and reference captions\nfor i in range(len(generated_captions)):\n    print(f\"Image {i+1} - Generated: {len(generated_captions[i])}, Reference: {len(reference_captions[i])}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:33.981160Z","iopub.execute_input":"2023-12-10T13:44:33.982056Z","iopub.status.idle":"2023-12-10T13:44:34.270534Z","shell.execute_reply.started":"2023-12-10T13:44:33.982016Z","shell.execute_reply":"2023-12-10T13:44:34.268583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\n# Assuming you have a function to generate captions similar to 'predict_caption'\n# predict_caption(model, image, tokenizer, max_length, features)\n\n# Create lists to store generated and reference captions\ngenerated_captions = []\nreference_captions = []\n\n# Generate captions and collect references\nfor index, record in samples.iterrows():\n    generated_caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n    generated_captions.append(generated_caption.split())  # Tokenize generated caption\n    reference_captions.append([record['caption'].split()])  # Tokenize reference caption\n\n# Compute BLEU score\nbleu_scores = []\nfor i in range(len(generated_captions)):\n    bleu_score = sentence_bleu(reference_captions[i], generated_captions[i])\n    bleu_scores.append(bleu_score)\n\n# Average BLEU score across all samples\navg_bleu_score = sum(bleu_scores) / len(bleu_scores)\nprint(\"Average BLEU Score:\", avg_bleu_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:34.271898Z","iopub.status.idle":"2023-12-10T13:44:34.273406Z","shell.execute_reply.started":"2023-12-10T13:44:34.273101Z","shell.execute_reply":"2023-12-10T13:44:34.273151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom nltk.util import ngrams\n\n# Function to calculate ROUGE-N scores\ndef rouge_n(reference, candidate, n=2):\n    reference_ngrams = list(ngrams(reference.split(), n))\n    candidate_ngrams = list(ngrams(candidate.split(), n))\n\n    # Calculate intersection (common ngrams)\n    intersection = len(set(reference_ngrams).intersection(candidate_ngrams))\n\n    # Calculate ROUGE-N precision and recall\n    precision = intersection / len(candidate_ngrams)\n    recall = intersection / len(reference_ngrams)\n\n    # Calculate ROUGE-N score (F1 score)\n    rouge_score = 2 * ((precision * recall) / (precision + recall + 1e-8))\n\n    return rouge_score\n\n# Example usage:\nreference_caption = \"This is a reference caption\"\ngenerated_caption = \"This is a generated caption\"\n\nrouge_2_score = rouge_n(reference_caption, generated_caption, n=2)\nprint(f\"ROUGE-2 score: {rouge_2_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T13:44:34.274857Z","iopub.status.idle":"2023-12-10T13:44:34.275781Z","shell.execute_reply.started":"2023-12-10T13:44:34.275615Z","shell.execute_reply":"2023-12-10T13:44:34.275633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}